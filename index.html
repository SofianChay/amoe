<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AMoE: Agglomerative Mixture-of-Experts</title>
    <!-- Add or increment the version query parameter -->
    <link rel="stylesheet" href="style.css?v=2.0">
    <script src="script.js?v=2.0" defer></script>
</head>
<body>

<header>
    <h1>AMoE</h1>
    <div class="subtitle" style="margin-bottom: 20px; color: #fff;">Agglomerative Mixture-of-Experts Vision Foundation Model</div>
    
    <div class="authors">
        Sofian Chaybouti, Sanath Narayan, Yasser Dahou, PhÃºc H. LÃª Khac, Ankit Singh, Ngoc Dung Huynh, Wamiq Reyaz Para, Hilde Kuehne, Hakim Hacid
    </div>
    
    <div class="links">
        <!-- Paper Link -->
        <a href="#" class="btn-link">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 17.732 24.269">
                <g transform="translate(-566.984 -271.548)">
                    <path d="M573.549,280.916l2.266,2.738,6.674-7.84c.353-.47.52-.717.353-1.117a1.218,1.218,0,0,0-1.061-.748h0a.953.953,0,0,0-.712.262Z"/>
                    <path d="M579.525,282.225l-10.606-10.174a1.413,1.413,0,0,0-.834-.5,1.09,1.09,0,0,0-1.027.66c-.167.4-.047.681.319,1.206l8.44,10.242h0l-6.282,7.716a1.336,1.336,0,0,0-.323,1.3,1.114,1.114,0,0,0,1.04.69A.992.992,0,0,0,571,293l8.519-7.92A1.924,1.924,0,0,0,579.525,282.225Z"/>
                    <path d="M584.32,293.912l-8.525-10.275,0,0L573.53,280.9l-1.389,1.254a2.063,2.063,0,0,0,0,2.965l10.812,10.419a.925.925,0,0,0,.742.282,1.039,1.039,0,0,0,.953-.667A1.261,1.261,0,0,0,584.32,293.912Z"/>
                </g>
            </svg>
            Paper 
        </a>

        <!-- Code Link -->
        <a href="#" class="btn-link">
            <svg viewBox="0 0 24 24">
                <path d="M12 0C5.37 0 0 5.37 0 12c0 5.31 3.435 9.795 8.205 11.385.6.105.825-.255.825-.57 0-.285-.015-1.04-.015-2.04-3.338.72-4.042-1.61-4.042-1.61-.546-1.385-1.335-1.755-1.335-1.755-1.087-.745.085-.73.085-.73 1.2.085 1.83 1.235 1.83 1.235 1.07 1.835 2.805 1.305 3.495.995.105-.775.42-1.305.765-1.605-2.665-.3-5.466-1.335-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.3-.54-1.525.105-3.175 0 0 1.005-.315 3.3 1.23.96-.27 1.98-.405 3-.405s2.04.135 3 .405c2.295-1.56 3.3-1.23 3.3-1.23.645 1.655.24 2.875.12 3.175.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.435.375.81 1.095.81 2.22 0 1.605-.015 2.895-.015 3.285 0 .315.225.69.825.57C20.565 21.795 24 17.31 24 12c0-6.63-5.37-12-12-12z"/>
            </svg>
            Code
        </a>

        <!-- Models Link -->
        <a href="#" class="btn-link">
            <span style="font-size: 1.2em; margin-right: 5px;">ðŸ¤—</span>
            Models
        </a>

        <!-- OpenLVD Link -->
        <a href="#" class="btn-link">
            <svg viewBox="0 0 24 24">
                <path d="M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zm-5 14H7v-2h7v2zm3-4H7v-2h10v2zm0-4H7V7h10v2z"/>
            </svg>
            OpenLVD Dataset
        </a>
    </div>
</header>

<section id="overview">
    <h2>Overview</h2>
    <div class="figure-container">
        <img src="figures/main_fig_arxiv%20(2).png" alt="AMoE Vision Foundation Model">
        <p class="caption">
            <strong>FIG. 1:</strong> A Mixture-of-Experts student is distilled from multiple frozen vision teachers (SigLIP2 and DINOv3).
        </p>
    </div>

    <p class="abstract-text">
        Vision foundation models trained via multi-teacher distillation offer a promising path toward unified visual representations. We introduce <strong>AMoE</strong> (Agglomerative Mixture-of-Experts), which distills knowledge from <strong>SigLIP2</strong> and <strong>DINOv3</strong> simultaneously into a single Mixture-of-Experts student.
    </p>
    <ul>
        <li><strong>OpenLVD200M:</strong> A curated 200M-image corpus, substantially improving sample efficiency over random sampling.</li>
        <li><strong>Token-balanced batching:</strong> Stabilizes representation learning across resolutions.</li>
        <li><strong>Asymmetric Relation-Knowledge Distillation (ARKD):</strong> Preserves the geometric properties of each teacher while enabling effective knowledge transfer.</li>
    </ul>
    <p>
        Instantiated in a Mixture-of-Experts backbone, AMoE sets a new state-of-the-art on global representation and retrieval benchmarks while using significantly fewer tokens than competitors like RADIOv2.5.
    </p>
</section>

<section id="method">
    <h2>AMoE Framework</h2>
    
    <h3>// State-of-the-Art Comparison</h3>
    <p>AMoE outperforms RADIOv2.5-H while using significantly fewer tokens (4.7x less).</p>
    <table>
        <thead>
            <tr>
                <th>Method</th>
                <th>Image-Text Avg (Top-1)</th>
                <th>kNN Avg (Top-1)</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>RADIOv2.5-H (Ensemble)</td>
                <td>82.26</td>
                <td>84.42</td>
            </tr>
            <tr>
                <td class="highlight">AMoE (Ensemble)</td>
                <td class="highlight">84.13</td>
                <td class="highlight">87.44</td>
            </tr>
        </tbody>
    </table>

    <h3>// Ensembling Strategy</h3>
    <p>
        To leverage the complementary strengths of our teachers, we employ an entropy-weighted head-ensembling strategy at test time. For each input, we compute the entropy of the prediction distribution from each teacher head (SigLIP2 and DINOv3). Heads with lower entropy (higher confidence) are assigned higher weights.
    </p>
    
    <h3>// Key Contributions</h3>
    <div class="flex-row">
        <div class="flex-col">
            <h4>01. OpenLVD200M Dataset</h4>
            <p>
                We introduce <strong>OpenLVD200M</strong>, a curated 200M-image corpus constructed via hierarchical clustering. It provides balanced coverage of visual concepts.
            </p>
        </div>
        <div class="flex-col">
            <h4>02. MoE Architecture</h4>
            <p>
                We use a Mixture-of-Experts (MoE) backbone to effectively absorb complementary signals from different teachers.
            </p>
        </div>
        <div class="flex-col">
            <h4>03. Token-Balanced Batching</h4>
            <p>
                We employ token-balanced batching to stabilize training across varying resolutions, packing images to a fixed token budget.
            </p>
        </div>
        <div class="flex-col">
            <h4>04. Asymmetric RKD</h4>
            <p>
                We introduce ARKD to preserve the pairwise geometry of teacher embeddings. Unlike standard relational distillation, ARKD is asymmetric.
            </p>
        </div>
    </div>

    <div class="figure-container">
        <img src="figures/token_balanced_batching%20(3).png" alt="Token Balanced Batching">
        <p class="caption">
            <strong>FIG. 2:</strong> Packing multiple native-resolution images per sequence up to a fixed token budget prevents low-res forgetting and improves performance.
        </p>
    </div>
</section>

<section id="dataset">
    <h2>OpenLVD200M Dataset</h2>
    
    <h3>// Construction</h3>
    <p>
        We construct OpenLVD200M to mitigate the long-tail distribution inherent in web-scale data. Inspired by the hierarchical clustering and sampling technique from <em>Vo et al. (2024)</em>, we process a 2.3B-image pool (combining DFN and LAION) to learn a semantic hierarchy of visual concepts. By sampling uniformly across these semantic clusters rather than the raw data distribution, we ensure balanced coverage of both common and rare concepts.
    </p>

    <h3>// Ablation: OpenLVD vs. Random Sampling</h3>
    <table>
        <thead>
            <tr>
                <th>Method</th>
                <th>Image-Text Avg</th>
                <th>kNN Avg</th>
                <th>T2I</th>
                <th>I2T</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>Random (200M)</td>
                <td>74.96</td>
                <td>82.66</td>
                <td>57.63</td>
                <td>75.12</td>
            </tr>
            <tr>
                <td class="highlight">OpenLVD200M</td>
                <td class="highlight">79.11</td>
                <td class="highlight">85.08</td>
                <td class="highlight">59.14</td>
                <td class="highlight">76.43</td>
            </tr>
        </tbody>
    </table>
</section>

<section id="analysis">
    <h2>In-Depth Analysis</h2>

    <h3>1. RoPE Impact: Golden vs Axial</h3>
    <p>
        We evaluate the generalization capabilities of our Golden-RoPE strategy against standard Axial RoPE when extrapolating to unseen high resolutions.
    </p>

    <!-- Interactive RoPE Figure -->
    <div class="figure-container interactive-figure">
        <div class="figure-header">
            <div class="sample-nav">
                <button id="prevRoPESample" class="nav-btn">&lt; PREV</button>
                <span id="ropeSampleIndicator">SAMPLE 1/3</span>
                <button id="nextRoPESample" class="nav-btn">NEXT &gt;</button>
            </div>
        </div>
        
        <div class="rope-grid">
            <div class="grid-header empty"></div>
            <div class="grid-header">Original</div>
            <div class="grid-header">256x256</div>
            <div class="grid-header">768x768</div>
            <div class="grid-header">2048x2048</div>

            <!-- Golden Row -->
            <div class="grid-label">
                <div class="label-text">Normalized Coordinates<br></div>
            </div>
            <div class="grid-img rowspan-2"><img id="rope-original" src="" alt="Original Image"></div>
            <div class="grid-img"><img id="golden-256" src="" alt="Golden 256"></div>
            <div class="grid-img"><img id="golden-768" src="" alt="Golden 768"></div>
            <div class="grid-img"><img id="golden-2048" src="" alt="Golden 2048"></div>

            <!-- Axial Row -->
            <div class="grid-label">
                <div class="label-text">Standard Axial RoPE<br></div>
            </div>
            <!-- Original image spans 2 rows, so no img here -->
            <div class="grid-img"><img id="axial-256" src="" alt="Axial 256"></div>
            <div class="grid-img"><img id="axial-768" src="" alt="Axial 768"></div>
            <div class="grid-img"><img id="axial-2048" src="" alt="Axial 2048"></div>
        </div>

        <p class="caption">
            <strong>FIG. 3:</strong> Comparison of feature map consistency across resolutions. Golden RoPE (normalized coordinates) maintains strong scale invariance and feature coherence even at unseen resolutions ($2048 \times 2048$), whereas standard Axial RoPE degrades. Use arrows to switch samples.
        </p>
    </div>
    
    <h3>2. PCA Visualizations</h3>
    <p>
        We provide a qualitative comparison of the learned student representations against the original teacher features.
    </p>
    
    <!-- Interactive PCA Figure -->
    <div class="figure-container interactive-figure">
        <div class="figure-header">
            <div class="sample-nav">
                <button id="prevPCASample" class="nav-btn">&lt; PREV</button>
                <span id="pcaSampleIndicator">SAMPLE 1/5</span>
                <button id="nextPCASample" class="nav-btn">NEXT &gt;</button>
            </div>
        </div>
        
        <div class="pca-grid">
            <!-- Headers -->
            <div class="grid-header">Original Image</div>
            <div class="grid-header">AMoE Map</div>
            
            <!-- Original & AMoE -->
            <div class="grid-img"><img id="pca-original" src="" alt="Original Image"></div>
            <div class="grid-img"><img id="pca-amoe" src="" alt="AMoE Map"></div>

            <!-- Headers for Teachers -->
            <div class="grid-header">DINOv3 Teacher</div>
            <div class="grid-header">DINOv3 Head (AMoE)</div>

            <!-- DINO Row -->
            <div class="grid-img"><img id="pca-dino" src="" alt="DINOv3 Teacher"></div>
            <div class="grid-img"><img id="pca-dino-head" src="" alt="DINOv3 Head"></div>

            <!-- Headers for Teachers -->
            <div class="grid-header">SigLIP2 Teacher</div>
            <div class="grid-header">SigLIP2 Head (AMoE)</div>

            <!-- SigLIP Row -->
            <div class="grid-img"><img id="pca-siglip" src="" alt="SigLIP2 Teacher"></div>
            <div class="grid-img"><img id="pca-siglip-head" src="" alt="SigLIP2 Head"></div>
        </div>

        <p class="caption">
            <strong>FIG. 4:</strong> PCA maps showing the student (AMoE) closely reconstructing the teacher distributions. The student retains SigLIP2's text-aware features and DINOv3's geometric consistency. Use arrows to switch samples.
        </p>
    </div>

    <h3>3. Register PHI-S Impact</h3>
    <p>
        We analyze the effectiveness of PHI-S normalization on different token types. While effective for global and patch tokens, we find that the first register token in DINOv3 exhibits a multi-mode distribution.
    </p>
    <div class="figure-container">
        <img src="figures/phis_pca_maps.png" alt="PHI-S PCA Maps">
        <p class="caption">
            <strong>FIG. 5:</strong> The first register (Row 4) exhibits multi-mode distributions which standard PHI-S fails to normalize correctly, leading to training instability.
        </p>
    </div>

</section>

<section id="results">
    <h2>Results & Ablations</h2>
    
    <h3>// Retrieval Performance (Recall@1)</h3>
    <table>
        <thead>
            <tr>
                <th>Method</th>
                <th colspan="2">MSCOCO5k</th>
                <th colspan="2">Flickr30k</th>
            </tr>
            <tr>
                <th></th>
                <th>T2I</th>
                <th>I2T</th>
                <th>T2I</th>
                <th>I2T</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>RADIOv2.5-H (Ens)</td>
                <td>53.24</td>
                <td>71.82</td>
                <td>80.96</td>
                <td>93.50</td>
            </tr>
            <tr>
                <td class="highlight">AMoE (Ens)</td>
                <td class="highlight">53.98</td>
                <td class="highlight">72.14</td>
                <td class="highlight">81.20</td>
                <td class="highlight">94.30</td>
            </tr>
        </tbody>
    </table>

    <h3>// Ablation: Impact of ARKD</h3>
    <table>
        <thead>
            <tr>
                <th>Loss Variant</th>
                <th>Image-Text Avg</th>
                <th>kNN Avg</th>
                <th>MSCOCO T2I</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>Vanilla MT (No RKD)</td>
                <td>77.62</td>
                <td>83.54</td>
                <td>48.15</td>
            </tr>
            <tr>
                <td>Symmetric RKD</td>
                <td>79.49</td>
                <td>82.61</td>
                <td>48.32</td>
            </tr>
            <tr>
                <td class="highlight">Asymmetric RKD (Ours)</td>
                <td class="highlight">80.21</td>
                <td class="highlight">83.63</td>
                <td class="highlight">48.51</td>
            </tr>
        </tbody>
    </table>
</section>

<footer>
    <p>// AMoE Project &copy; 2026</p>
</footer>

</body>
</html>